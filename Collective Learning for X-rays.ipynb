{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will demonstrate the Collective Learning Framework in PySyft. First let's import some things and define some training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f3ac8c6d7b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from random import randint\n",
    "import random as rand\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as nn_func\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import syft as sy\n",
    "from torchsummary import summary\n",
    "\n",
    "hook = sy.TorchHook(torch)\n",
    "\n",
    "\n",
    "class Arguments:\n",
    "    def __init__(self):\n",
    "        self.batch_size = 8\n",
    "        self.n_batches_for_vote = 13\n",
    "        self.test_batch_size = 16\n",
    "        self.epochs = 15\n",
    "        self.steps_per_epoch = 10\n",
    "        self.lr = 0.01\n",
    "        # self.momentum = 0.5  # momentum is not supported by pysyft\n",
    "        self.seed = 1\n",
    "        self.log_interval = 1\n",
    "        self.n_hospitals = 5\n",
    "        self.vote_threshold = (self.n_hospitals - 1) // 2\n",
    "        self.train_ratio = 0.92\n",
    "        self.data_dir = \"./data/chest_x-ray\"\n",
    "        self.pos_weight = torch.tensor([0.27])  # there is about 1 normal to every 3 pneu\n",
    "        \n",
    "args = Arguments()\n",
    "torch.manual_seed(args.seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create PySyft VirtualWorkers to represent each hospital."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hospitals = []\n",
    "for i in range(args.n_hospitals):\n",
    "    hospitals.append(sy.VirtualWorker(hook, id=\"hospital \" + str(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to create a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 128, 128]             320\n",
      "       BatchNorm2d-2         [-1, 32, 128, 128]              64\n",
      "            Conv2d-3           [-1, 64, 32, 32]          18,496\n",
      "       BatchNorm2d-4           [-1, 64, 32, 32]             128\n",
      "            Linear-5                    [-1, 1]              65\n",
      "================================================================\n",
      "Total params: 19,073\n",
      "Trainable params: 19,073\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.06\n",
      "Forward/backward pass size (MB): 9.00\n",
      "Params size (MB): 0.07\n",
      "Estimated Total Size (MB): 9.14\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, (3, 3), padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, (3, 3), padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.fc1 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn_func.relu(self.conv1(x))\n",
    "        x = self.bn1(x)\n",
    "        x = nn_func.max_pool2d(x, kernel_size=(4, 4))\n",
    "        x = nn_func.relu(self.conv2(x))\n",
    "        x = self.bn2(x)\n",
    "        x = nn_func.max_pool2d(x, kernel_size=(32, 32))\n",
    "        x = x.view(-1, 64)\n",
    "        x = self.fc1(x)\n",
    "        return x  # NB: output is in *logits*\n",
    "    \n",
    "model = Net()\n",
    "summary(model, input_size=(1, 128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally a federated dataset with the x-ray data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XrayDataset(Dataset):\n",
    "    \"\"\"X-ray dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data_dir, width=128, height=128, seed=42, transform=None, train=True, train_ratio=0.96):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            data_dir (string): Path to the data directory.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.width, self.height = width, height\n",
    "        self.cases = list(Path(data_dir).rglob('*.jp*'))  # list of filenames\n",
    "        if len(self.cases) == 0:\n",
    "            raise Exception(\"No data foud in path: \" + str(data_dir))\n",
    "\n",
    "        rand.seed(seed)\n",
    "        rand.shuffle(self.cases)\n",
    "\n",
    "        n_cases = int(train_ratio * len(self.cases))\n",
    "        assert (n_cases > 0), \"There are no cases\"\n",
    "        if train:\n",
    "            self.cases = self.cases[:n_cases]\n",
    "        else:\n",
    "            self.cases = self.cases[n_cases:]\n",
    "\n",
    "        self.diagnosis = []  # list of filenames\n",
    "        self.normal_data = []\n",
    "        self.pneumonia_data = []\n",
    "        for case in self.cases:\n",
    "            if 'NORMAL' in str(case):\n",
    "                self.diagnosis.append(0)\n",
    "                self.normal_data.append(case)\n",
    "            elif 'PNEUMONIA' in str(case):\n",
    "                self.diagnosis.append(1)\n",
    "                self.pneumonia_data.append(case)\n",
    "            else:\n",
    "                print(case, \" - has invalid category\")\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.cases)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # todo: balanced classes\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        else:\n",
    "            idx = [idx]\n",
    "\n",
    "        batch_size = len(idx)\n",
    "\n",
    "        # Define two numpy arrays for containing batch data and labels\n",
    "        batch_data = np.zeros((batch_size, self.width, self.height), dtype=np.float32)\n",
    "        batch_labels = np.zeros(batch_size, dtype=np.float32)\n",
    "\n",
    "        for j, index in enumerate(idx):\n",
    "            batch_data[j] = self.to_rgb_normalize_and_resize(self.cases[index], self.width, self.height)\n",
    "            batch_labels[j] = self.diagnosis[index]\n",
    "\n",
    "        sample = (batch_data, batch_labels)\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    @staticmethod\n",
    "    def to_rgb_normalize_and_resize(filename, width, height):\n",
    "        img = cv2.imread(str(filename))\n",
    "        img = cv2.resize(img, (width, height))\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        img = img.astype(np.float32) / 255.\n",
    "\n",
    "        return img\n",
    "    \n",
    "# make a federated data loader for training\n",
    "train_dataset = XrayDataset(args.data_dir, train_ratio=args.train_ratio)\n",
    "\n",
    "fed_train_dataset = train_dataset.federate(hospitals)\n",
    "fed_train_loader = sy.FederatedDataLoader(fed_train_dataset, batch_size=args.batch_size,\n",
    "                                          iter_per_worker=True, shuffle=True)\n",
    "\n",
    "# make an unfederated data loader for testing\n",
    "test_dataset = XrayDataset(args.data_dir, train=False, train_ratio=args.train_ratio)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=args.test_batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now we have a model, VirtualWorkers representing hospitals, and a training dataset that is split between the hospitals. The next step is to define the functions that will perform training and voting. In the Collective Learning scheme one worker is selected to train the model, and then the other workers vote on whether to accept the new model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colearn_train(args, model: Net,\n",
    "                  federated_train_loader: sy.FederatedDataLoader,\n",
    "                  optimizer, epoch, workers):\n",
    "    model.train()  # sets model to \"training\" mode. Does not perform training.\n",
    "\n",
    "    # pick a random hospital\n",
    "    proposer_index = randint(0, len(workers) - 1)\n",
    "    proposer = workers[proposer_index]\n",
    "    print(\"Proposer\", proposer_index, proposer)\n",
    "    model.send(proposer)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=args.pos_weight, reduction='mean')\n",
    "\n",
    "    # go through all the batches for hosp_n, perform training, get model back\n",
    "    for batch_idx, data_dict in enumerate(federated_train_loader):  # a distributed dataset\n",
    "        data, target = data_dict[proposer]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get()  # get the loss back\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size,\n",
    "                len(federated_train_loader) * args.batch_size,\n",
    "                100. * batch_idx * len(workers) / len(federated_train_loader), loss.item()))\n",
    "        if batch_idx == args.steps_per_epoch:\n",
    "            break\n",
    "\n",
    "    model.get()\n",
    "    return proposer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the voting function. The vote is based on the performance (AUC score) on batches of the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_on_training_set(args: Arguments, model, train_loader, workers):\n",
    "    model.eval()  # sets model to \"eval\" mode.\n",
    "    aucs = {}\n",
    "    batch_count = 0\n",
    "    all_targets = {w: np.array([]) for w in workers}\n",
    "    all_pred_probs = {w: np.array([]) for w in workers}\n",
    "    with torch.no_grad():\n",
    "        for data_dict in train_loader:\n",
    "            for worker, (data, target) in data_dict.items():\n",
    "                model.send(data.location)\n",
    "                output = model(data)\n",
    "                pred_prob = torch.sigmoid(output)\n",
    "                # NB: here we have to get the labels from the remote worker to the central execution location.\n",
    "                # This breaks data privacy, but we need to do it here because PySyft doesn't support calculation\n",
    "                # of AUC score on the remote worker.\n",
    "                all_targets[worker] = np.append(all_targets[worker], target.get().numpy())\n",
    "                all_pred_probs[worker] = np.append(all_pred_probs[worker], pred_prob.get().numpy())\n",
    "\n",
    "                model.get()\n",
    "            batch_count += 1\n",
    "            if batch_count == args.n_batches_for_vote:\n",
    "                break\n",
    "    for w in workers:\n",
    "        aucs[w] = roc_auc_score(all_targets[w], all_pred_probs[w])\n",
    "\n",
    "    return aucs\n",
    "\n",
    "\n",
    "def vote(model, args, old_performance, federated_train_loader, workers, proposer):\n",
    "    new_performance = test_on_training_set(args, model, federated_train_loader, workers)\n",
    "    print(\"Doing voting\")\n",
    "\n",
    "    votes = 0\n",
    "    for worker, old_performance in old_performance.items():\n",
    "        if worker != proposer:\n",
    "            if new_performance[worker] >= old_performance:\n",
    "                votes += 1\n",
    "                print(worker, \"votes yes\")\n",
    "            else:\n",
    "                print(worker, \"votes no\")\n",
    "    if votes >= args.vote_threshold:\n",
    "        print(\"Vote succeeded\")\n",
    "        return True, new_performance\n",
    "    else:\n",
    "        print(\"Vote failed\")\n",
    "        return False, new_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we almost have everything we need to run Collective Learning. The final tep is to define a function that will provide scores on the test set so that we can see the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, test_loader):\n",
    "    model.eval()  # sets model to \"eval\" mode.\n",
    "    test_loss = 0\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=args.pos_weight, reduction='sum')\n",
    "    all_targets = np.array([])\n",
    "    all_pred_probs = np.array([])\n",
    "    with torch.no_grad():\n",
    "        for i, (data, target) in enumerate(test_loader):\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # sum up batch loss\n",
    "            pred_prob = torch.sigmoid(output)\n",
    "            all_targets = np.append(all_targets, target.numpy())\n",
    "            all_pred_probs = np.append(all_pred_probs, pred_prob.numpy())\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    auc = roc_auc_score(all_targets, all_pred_probs)\n",
    "    print('\\nTest set: Average loss: {:.4f}, AUC: {}\\n'.format(\n",
    "        test_loss, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show initial accuracy\n",
    "test(args, model, test_loader)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "\n",
    "current_performance = {w: 0 for w in hospitals}\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    current_weights = model.state_dict()\n",
    "    proposer = colearn_train(args, model, fed_train_loader, optimizer, epoch, hospitals)\n",
    "    update_accepted, new_performance = vote(model, args, current_performance, fed_train_loader, hospitals, proposer)\n",
    "    if update_accepted:\n",
    "        current_performance = new_performance\n",
    "        test(args, model, test_loader)\n",
    "    else:\n",
    "        # load the old weights into the model\n",
    "        model.load_state_dict(current_weights)\n",
    "        \n",
    "print(\"Training complete\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
